{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d9d25-4a12-4409-8293-d552f48845eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# also look at: https://www.geeksforgeeks.org/how-to-implement-a-gradient-descent-in-python-to-find-a-local-minimum/\n",
    "# Importing Libraries\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd9fa4-e0f8-4d88-b339-7c978bc122c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Gradient Descent on a Defined Function (not linear regression points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e605ec1-961d-4ed1-9c68-f9d84afb37d0",
   "metadata": {},
   "source": [
    "## First define our overall gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d629a-37d7-47f4-8393-1583495c99d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient(func_to_min, deriv_func_to_min, gradient_descent, learning_rate = 0.01, initial_guess = 0, num_iterations = 100, threshold = 0.001):\n",
    "    \n",
    "    # Hyperparams from function inputs\n",
    "    learning_rate = learning_rate\n",
    "    initial_guess = initial_guess\n",
    "    num_iterations = num_iterations\n",
    "    threshold = threshold\n",
    "    \n",
    "    # the following three are outside functions so that they can be changed as we want\n",
    "    def call_function_to_minimize(x):\n",
    "        return func_to_min(x)\n",
    "\n",
    "    def call_derivative_of_function(x):\n",
    "        return deriv_func_to_min(x)\n",
    "    \n",
    "    def call_gradient_descent(deriv_func_to_min, learning_rate, initial_guess, num_iterations, threshold):\n",
    "        return gradient_descent(deriv_func_to_min, learning_rate, initial_guess, num_iterations, threshold)\n",
    "\n",
    "    # Run gradient descent\n",
    "    minimum, total_iterations = call_gradient_descent(call_derivative_of_function, learning_rate, initial_guess,\\\n",
    "                                                      num_iterations, threshold)\n",
    "\n",
    "    # print(\"Minimum value found at x =\", minimum)\n",
    "    # print(\"Minimum function value =\", function_to_minimize(minimum))\n",
    "    # print(total_iterations)\n",
    "    \n",
    "    return total_iterations, minimum, call_function_to_minimize(minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac464589-bac3-4224-a887-a3062073e76b",
   "metadata": {},
   "source": [
    "## Use the gradient function and first use constant descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058a391f-f3ed-48a0-85a5-82a81a7ac573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to minimize\n",
    "def function_to_minimize(x):\n",
    "    return x**2 + 5*x + 6\n",
    "\n",
    "# Define the derivative of the function\n",
    "def derivative_of_function(x):\n",
    "    return 2*x + 5\n",
    "\n",
    "# Gradient Descent function\n",
    "def constant_descent(deriv_func_to_min, learning_rate, initial_guess, num_iterations, threshold):\n",
    "    \n",
    "    x = initial_guess\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        prev_guess = x\n",
    "        gradient = deriv_func_to_min(x)\n",
    "        x = x - learning_rate * gradient\n",
    "\n",
    "        if iteration > 0 and abs(x - prev_guess) <= threshold:\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return x, iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a8113-f73d-42b1-aeb0-83ec0b468077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs = 50\n",
    "\n",
    "learning_rate = np.random.uniform(0, 0.005, (runs, ))\n",
    "initial_guesses = np.random.uniform(-20, 20, (runs, ))\n",
    "iterations_list = []\n",
    "\n",
    "max_iterations = 10_000\n",
    "max_threshold = 0.0001\n",
    "\n",
    "for guess in initial_guesses:\n",
    "    temp_list = []\n",
    "\n",
    "    for rate in learning_rate:\n",
    "        temp_list.append(gradient(function_to_minimize, derivative_of_function, constant_descent, learning_rate = rate,\\\n",
    "                                  initial_guess = guess, num_iterations = max_iterations, threshold = max_threshold)[0])\n",
    "    \n",
    "    # this will create a list of lists as z-values so we can graph\n",
    "    iterations_list.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8756d1a-74da-4069-b8ed-fec693769217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save variables for our constant learning rate section\n",
    "const_learning_rate = learning_rate\n",
    "const_initial_guesses = initial_guesses\n",
    "const_iterations_list = iterations_list\n",
    "\n",
    "# each median is for initial_guesses\n",
    "const_medians = [np.median(iters) for iters in const_iterations_list]\n",
    "\n",
    "const_total_median = np.median(const_medians)\n",
    "const_total_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8500d0-69bc-4047-85c8-1de60f8f6a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the graph interactive\n",
    "%matplotlib widget \n",
    "\n",
    "X = np.array(learning_rate)\n",
    "Y = np.array(initial_guesses)\n",
    "\n",
    "x, y = np.meshgrid(X, Y)\n",
    "z = np.array(iterations_list)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "#ax.plot_surface(x, y, z, cmap=cm.coolwarm,)\n",
    "ax.scatter(x, y, z)\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title('Iterations vs. Learning Rate and Initial Guesses')\n",
    "\n",
    "# Adding labels\n",
    "ax.set_xlabel('Learning Rate')\n",
    "ax.set_ylabel('Initial Guesses')\n",
    "ax.set_zlabel('Iterations')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1af681-8117-4f2e-a8a6-6a337dd185bf",
   "metadata": {},
   "source": [
    "## Create a descent function with random learning rates now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3378a-ba1d-45f6-8b0c-56eb0d43d926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the function to minimize\n",
    "def function_to_minimize(x):\n",
    "    return x**2 + 5*x + 6\n",
    "\n",
    "# Define the derivative of the function\n",
    "def derivative_of_function(x):\n",
    "    return 2*x + 5\n",
    "\n",
    "# Gradient Descent function\n",
    "def random_descent(deriv_func_to_min, learning_rate, initial_guess, num_iterations, threshold):\n",
    "    \n",
    "    #np.random.seed(8567) # include for reproduction of results\n",
    "    \n",
    "    x = initial_guess\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        prev_guess = x\n",
    "        gradient = deriv_func_to_min(x)\n",
    "        \n",
    "        # this is where the randomness is being included - we need to make sure that we get a positive number here so clip any negative values\n",
    "        rnd_learning_rate = np.random.normal(learning_rate, learning_rate/2)\n",
    "        while rnd_learning_rate <= 0:\n",
    "            rnd_learning_rate = np.random.normal(learning_rate, learning_rate/2)\n",
    "        \n",
    "        x = x - rnd_learning_rate * gradient\n",
    "\n",
    "        if iteration > 0 and abs(x - prev_guess) <= threshold:\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return x, iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a09bf71-e1fd-4df9-9e86-53b3ffb5a95d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs = 50\n",
    "\n",
    "learning_rate = np.random.uniform(0, 0.005, (runs, )) # learning rate is random but will use these as a baseline\n",
    "initial_guesses = np.random.uniform(-20, 20, (runs, ))\n",
    "iterations_list = []\n",
    "\n",
    "max_iterations = 10_000\n",
    "max_threshold = 0.0001\n",
    "\n",
    "for guess in initial_guesses:\n",
    "    temp_list = []\n",
    "\n",
    "    for rate in learning_rate:\n",
    "        temp_list.append(gradient(function_to_minimize, derivative_of_function, random_descent, learning_rate = rate,\\\n",
    "                                  initial_guess = guess, num_iterations = max_iterations, threshold = max_threshold)[0])\n",
    "    \n",
    "    # this will create a list of lists as z-values so we can graph\n",
    "    iterations_list.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c750a-584c-491e-a8fd-a2935137f0d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save variables for our random learning rate section\n",
    "rnd_learning_rate = learning_rate\n",
    "rnd_initial_guesses = initial_guesses\n",
    "rnd_iterations_list = iterations_list\n",
    "\n",
    "# each median is for initial_guesses\n",
    "rnd_medians = [np.median(iters) for iters in rnd_iterations_list]\n",
    "\n",
    "rnd_total_median = np.median(rnd_medians)\n",
    "rnd_total_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed23b72-5601-4ae3-bb7f-d6292c81c7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the graph interactive\n",
    "%matplotlib widget \n",
    "\n",
    "X = np.array(learning_rate)\n",
    "Y = np.array(initial_guesses)\n",
    "\n",
    "x, y = np.meshgrid(X, Y)\n",
    "z = np.array(iterations_list)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "#ax.plot_surface(x, y, z, cmap=cm.coolwarm,)\n",
    "ax.scatter(x, y, z)\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title('Iterations vs. Learning Rate and Initial Guesses')\n",
    "\n",
    "# Adding labels\n",
    "ax.set_xlabel('Learning Rate')\n",
    "ax.set_ylabel('Initial Guesses')\n",
    "ax.set_zlabel('Iterations')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8982fe26-e710-4473-afaa-dfdc968f92be",
   "metadata": {},
   "source": [
    "## Look at comparisons between the two on 2D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836cb68-d69d-4403-8527-5cff1f01a85f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data of constant learning rate\n",
    "x1 = const_initial_guesses\n",
    "y1 = const_medians\n",
    "\n",
    "# data of random initial guesses\n",
    "x2 = rnd_initial_guesses\n",
    "y2 = rnd_medians\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create scatter plots\n",
    "ax.scatter(x1, y1, color='blue', label='Constant Rate')\n",
    "ax.scatter(x2, y2, color='red', label='Random Rate')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Initial Guesses')\n",
    "ax.set_ylabel('Iterations')\n",
    "ax.set_title('Scatter Plot on Iterations vs. Initial Guesses')\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f0aa5f-6e3b-4cc7-9104-9c4633835c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data of constant learning rate\n",
    "x1 = const_learning_rate\n",
    "y1 = const_medians\n",
    "\n",
    "# data of random learning rate\n",
    "x2 = rnd_learning_rate\n",
    "y2 = rnd_medians\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create scatter plots\n",
    "ax.scatter(x1, y1, color='blue', label='Constant Rate')\n",
    "ax.scatter(x2, y2, color='red', label='Random Rate')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Learning Rates')\n",
    "ax.set_ylabel('Iterations')\n",
    "ax.set_title('Scatter Plot on Iterations vs. Learning Rates')\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92683b3e-59ff-40f5-b00d-e2015a080a0a",
   "metadata": {},
   "source": [
    "## Comparisons on a 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d0db2-88b8-4262-856b-fdc6d9c6e3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the graph interactive\n",
    "%matplotlib widget \n",
    "\n",
    "# constant learning rates\n",
    "X1 = np.array(const_learning_rate)\n",
    "Y1 = np.array(const_initial_guesses)\n",
    "\n",
    "x1, y1 = np.meshgrid(X1, Y1)\n",
    "z1 = np.array(const_iterations_list)\n",
    "\n",
    "# random learning rates\n",
    "X2 = np.array(rnd_learning_rate)\n",
    "Y2 = np.array(rnd_initial_guesses)\n",
    "\n",
    "x2, y2 = np.meshgrid(X2, Y2)\n",
    "z2 = np.array(rnd_iterations_list)\n",
    "\n",
    "# create figure and axes\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(x1, y1, z1, color='blue', label='Constant Rate')\n",
    "ax.scatter(x2, y2, z2, color='red', label='Random Rate')\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title('Scatter Plot on Iterations vs. Learning Rates vs. Initial Guesses')\n",
    "\n",
    "# Adding labels\n",
    "ax.set_xlabel('Learning Rate')\n",
    "ax.set_ylabel('Initial Guesses')\n",
    "ax.set_zlabel('Iterations')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ecd08-1ac4-40e4-a0c3-bd552b3fbfcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gradient Descent on Linear Regression Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d68e4f-03d4-4e3e-b6d2-6a278041d070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initial random points to put into the gradient descent function\n",
    "def rand_points(low_rand, high_rand):\n",
    "    \n",
    "    num_points = 100\n",
    "    X_vals = np.random.uniform(low=0, high=1, size=num_points)  # random x-value points\n",
    "    Y_func = lambda x: np.random.uniform(low=low_rand, high=high_rand, size=1) * x  # use this to get a slope on the random points\n",
    "\n",
    "    # create a dataframe of the random X and Y values\n",
    "    data = pd.DataFrame({\n",
    "        'X': X_vals,\n",
    "        'Y': Y_func(X_vals) + np.random.uniform(low=-1, high=1, size=num_points)  # function plus or minus a random amount for noise\n",
    "    })\n",
    "\n",
    "    # break the data into X and Y values\n",
    "    rand_X_vals = data['X']\n",
    "    rand_Y_vals = data['Y']\n",
    "    \n",
    "    return rand_X_vals, rand_Y_vals\n",
    "\n",
    "# get the \"true\" values using sklearn to check our gradient descent calculations\n",
    "def grad_reg_sk(X, Y):\n",
    "\n",
    "    # reshape the X values so that we can use them in the regression - need a 2D array\n",
    "    X_reshape = X.values.reshape(-1, 1)\n",
    "\n",
    "    # run the actual regression\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(X_reshape, Y)\n",
    "    Y_pred_sk = regressor.predict(X_reshape)\n",
    "\n",
    "    # get the slope and intercept of the regression run\n",
    "    slope = regressor.coef_[0]\n",
    "    intercept = regressor.intercept_\n",
    "\n",
    "    # print(f'Slope is: {slope} and Intercept is: {intercept}')\n",
    "\n",
    "    # plot the information of the data points and the regression line\n",
    "    # plt.scatter(X_reshape, Y, color = 'red')\n",
    "    # plt.plot(X_reshape, Y_pred_sk, color = 'blue')\n",
    "    # plt.xlabel('X')\n",
    "    # plt.ylabel('Y')\n",
    "    # plt.show()\n",
    "    \n",
    "    return slope, intercept\n",
    "\n",
    "# rejecting outliers - template from chatgpt\n",
    "def reject_outliers(index_arr, data_arr, m=3):\n",
    "    \n",
    "    # calculate z-scores for each point in our array\n",
    "    z_scores = np.abs((data_arr - np.median(data_arr)) / np.std(data_arr))\n",
    "\n",
    "    # find the indices of outliers\n",
    "    outlier_indices = np.where(z_scores > m)[0]\n",
    "    \n",
    "    # remove outliers from the y-values and from the corresponding x-values\n",
    "    index_arr = np.delete(index_arr, outlier_indices)\n",
    "    new_data_arr = np.delete(data_arr, outlier_indices)\n",
    "\n",
    "    return index_arr, new_data_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc00df32-0414-4cf5-b7e2-faeabdba242a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The below is run based on a set number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd893a-9800-4609-b91e-ac55621cc73a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
    "# traditional gradient descent\n",
    "def grad_reg(X, Y, learning_rate, iterations):\n",
    "    \n",
    "    # initial slope and intercept guesses\n",
    "    m = 0\n",
    "    c = 0\n",
    "\n",
    "    learn_rate = learning_rate  # learning rate\n",
    "    epochs = iterations  # iterations of gradient descent\n",
    "\n",
    "    n = int(len(X))  # for use in the derivatives later on\n",
    "\n",
    "    # gradient descent iterations\n",
    "    for i in range(epochs):\n",
    "\n",
    "        # current predicted value of Y\n",
    "        Y_pred = (m * X) + c\n",
    "\n",
    "        # derivatives\n",
    "        D_m = (-2/n) * sum(X * (Y - Y_pred))  # deriv wrt m\n",
    "        D_c = (-2/n) * sum(Y - Y_pred)  # deriv wrt c\n",
    "\n",
    "        # update the values\n",
    "        m = m - (learn_rate * D_m)  # updating m\n",
    "        c = c - (learn_rate * D_c)  # updating c\n",
    "\n",
    "    # print(f'Slope is: {m} and Intercept is: {c}')\n",
    "\n",
    "    # predictions based on the regression line calculated\n",
    "    Y_pred = (m * X) + c\n",
    "\n",
    "    # plot the information of the data points and the regression line\n",
    "    # plt.scatter(X, Y) \n",
    "    # plt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='red')  # regression line\n",
    "    # plt.show()\n",
    "    \n",
    "    return m, c\n",
    "\n",
    "# randomized gradient descent\n",
    "def grad_reg_rand(X, Y, learning_rate, iterations):\n",
    "    \n",
    "    # initial slope and intercept guesses\n",
    "    m = 0\n",
    "    c = 0\n",
    "\n",
    "    learn_rate = learning_rate  # this is used as a learning rate base to get distribution numbers around\n",
    "    epochs = iterations  # iterations of gradient descent\n",
    "\n",
    "    n = int(len(X))  # for use in the derivatives later on\n",
    "\n",
    "    # gradient descent iterations\n",
    "    for i in range(epochs):\n",
    "\n",
    "        # current predicted value of Y\n",
    "        Y_pred = (m * X) + c\n",
    "\n",
    "        # derivatives\n",
    "        D_m = (-2/n) * sum(X * (Y - Y_pred))  # deriv wrt m\n",
    "        D_c = (-2/n) * sum(Y - Y_pred)  # deriv wrt c\n",
    "        \n",
    "        # get the random learning rate for this iteration\n",
    "        # this is where the randomness is being included - make sure that we get a positive number here so clip negative values\n",
    "        rnd_learn_rate = np.random.normal(learn_rate, learn_rate/2)\n",
    "        while rnd_learn_rate <= 0:\n",
    "            rnd_learn_rate = np.random.normal(learn_rate, learn_rate/2)\n",
    "\n",
    "        # update the values\n",
    "        m = m - (rnd_learn_rate * D_m)  # updating m\n",
    "        c = c - (rnd_learn_rate * D_c)  # updating c\n",
    "\n",
    "    # print(f'Slope is: {m} and Intercept is: {c}')\n",
    "\n",
    "    # predictions based on the regression line calculated\n",
    "    Y_pred = (m * X) + c\n",
    "\n",
    "    # plot the information of the data points and the regression line\n",
    "    # plt.scatter(X, Y) \n",
    "    # plt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='red')  # regression line\n",
    "    # plt.show()\n",
    "    \n",
    "    return m, c\n",
    "\n",
    "def compare_grads(X, Y, learning_rate, iterations):\n",
    "    \n",
    "    # run the gradient descent algorithms\n",
    "    true_slope, true_int = grad_reg_sk(X, Y)  # run the sklearn regression to get the \"true\" numbers for comparison\n",
    "    slope_reg, int_reg = grad_reg(X, Y, learning_rate, iterations)  # run regular gradient descent\n",
    "    slope_rand, int_rand = grad_reg_rand(X, Y, learning_rate, iterations)  # run the randomized gradient descent\n",
    "    \n",
    "    # calculate the regular errors\n",
    "    slope_reg_err = (slope_reg - true_slope) / true_slope\n",
    "    int_reg_err = (int_reg - true_int) / true_int\n",
    "    \n",
    "    # calculate the rand errors\n",
    "    slope_rand_err = (slope_rand - true_slope) / true_slope\n",
    "    int_rand_err = (int_rand - true_int) / true_int\n",
    "    \n",
    "    return slope_reg_err, int_reg_err, slope_rand_err, int_rand_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12c8949-7da9-41a9-ab57-c47a94278da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run the comparisons\n",
    "reg_slope_errors = np.array([])\n",
    "reg_int_errors = np.array([])\n",
    "rand_slope_errors = np.array([])\n",
    "rand_int_errors = np.array([])\n",
    "\n",
    "iterations_used = 500\n",
    "\n",
    "# low rand and high rand are used when generating random X and Y data points\n",
    "low_rand = 1.8\n",
    "high_rand = 2.2\n",
    "\n",
    "# create an array of learning rates to go through\n",
    "learning_rate_arr = np.random.uniform(low=0.0001, high=0.001, size=50)\n",
    "\n",
    "# run our gradient descent under each learning rate\n",
    "for learning_rate in learning_rate_arr:\n",
    "    \n",
    "    # generate random X and Y values to use in our gradient descent\n",
    "    X_pts, Y_pts = rand_points(low_rand, high_rand)\n",
    "    \n",
    "    # runs the gradient descents and finds the errors of the values\n",
    "    grad_errors = compare_grads(X_pts, Y_pts, learning_rate, iterations_used)\n",
    "\n",
    "    # append the values to our arrays\n",
    "    reg_slope_errors = np.append(reg_slope_errors, grad_errors[0])\n",
    "    reg_int_errors = np.append(reg_int_errors, grad_errors[1])\n",
    "    rand_slope_errors = np.append(rand_slope_errors, grad_errors[2])\n",
    "    rand_int_errors = np.append(rand_int_errors, grad_errors[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d667db-9a9c-4067-baae-d5699b7d3875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the information for the errors of the slopes calculations\n",
    "# data of constant learning rate\n",
    "x1 = learning_rate_arr\n",
    "y1 = abs(reg_slope_errors)\n",
    "\n",
    "# data of random learning rate\n",
    "x2 = learning_rate_arr\n",
    "y2 = abs(rand_slope_errors)\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create scatter plots\n",
    "ax.scatter(x1, y1, color='blue', label='Constant Rate')\n",
    "ax.scatter(x2, y2, color='red', label='Random Rate')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Learning Rates')\n",
    "ax.set_ylabel(f'Slope Errors After {iterations_used} Iterations')\n",
    "ax.set_title('Scatter Plot on Error vs. Learning Rates for Slope')\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab1d7a-10a4-4a92-a006-07e20188e6d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the information for the errors of the intercept calculations\n",
    "# data of constant learning rate\n",
    "x3 = learning_rate_arr\n",
    "y3 = abs(reg_int_errors)\n",
    "\n",
    "# data of random learning rate\n",
    "x4 = learning_rate_arr\n",
    "y4 = abs(rand_int_errors)\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create scatter plots\n",
    "ax.scatter(x3, y3, color='blue', label='Constant Rate')\n",
    "ax.scatter(x4, y4, color='red', label='Random Rate')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Learning Rates')\n",
    "ax.set_ylabel(f'Intercept Errors After {iterations_used} Iterations')\n",
    "ax.set_title('Scatter Plot on Error vs. Learning Rates for Intercept')\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11864b80-0a12-4154-a270-e58c4bdc25b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# slope metrics for our comparison\n",
    "# checking number of times one beat the other\n",
    "print('Slope comparisons...')\n",
    "slope_compare = y2 - y1\n",
    "success_rate = np.count_nonzero(slope_compare < 0) / len(slope_compare)  # how many times the random algorithm won\n",
    "print(f'Random won {round(success_rate * 100, 2)}% of the time')\n",
    "\n",
    "# check means and medians\n",
    "slope_mean_compare = np.mean(y2) - np.mean(y1)\n",
    "slope_median_compare = np.median(y2) - np.median(y1)\n",
    "print(f'Random was lower (negative - better) or higher (positive - worse) by mean of {round(slope_mean_compare * 100, 2)}% and by median of {round(slope_median_compare * 100, 2)}%')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# intercept metrics for our comparison\n",
    "# checking number of times one beat the other\n",
    "print('Intercept comparisons...')\n",
    "intercept_compare = y4 - y3\n",
    "success_rate_int = np.count_nonzero(intercept_compare < 0) / len(intercept_compare)  # how many times the random algorithm won\n",
    "print(f'Random won {round(success_rate_int * 100, 2)}% of the time')\n",
    "\n",
    "# check means and medians\n",
    "intercept_mean_compare = np.mean(y4) - np.mean(y3)\n",
    "intercept_median_compare = np.median(y4) - np.median(y3)\n",
    "print(f'Random was lower (negative - better) or higher (positive - worse) by mean of {round(intercept_mean_compare * 100, 2)}% and by median of {round(intercept_median_compare * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b431b1ce-3fb9-4fa8-9a0d-d442ee5914d0",
   "metadata": {},
   "source": [
    "## The below is run based on finding the number of iterations until we get to a specific error threshold - this is only done with slope right now though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eafacb1-9ee4-44c5-a6a6-3cf5a97b6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
    "# traditional gradient descent\n",
    "def grad_reg_threshold(X, Y, learning_rate, threshold):\n",
    "    \n",
    "    # get the \"true\" values first for comparison\n",
    "    true_slope, true_int = grad_reg_sk(X, Y)\n",
    "    \n",
    "    # initial slope and intercept guesses\n",
    "    m = 0\n",
    "    c = 0\n",
    "\n",
    "    learn_rate = learning_rate  # learning rate\n",
    "    iteration_count = 0\n",
    "\n",
    "    n = int(len(X))  # for use in the derivatives later on\n",
    "\n",
    "    # gradient descent iterations until within threshold specified\n",
    "    while abs(true_slope - m) > threshold and abs(true_int - c) > threshold:\n",
    "\n",
    "        # current predicted value of Y\n",
    "        Y_pred = (m * X) + c\n",
    "\n",
    "        # derivatives\n",
    "        D_m = (-2/n) * sum(X * (Y - Y_pred))  # deriv wrt m\n",
    "        D_c = (-2/n) * sum(Y - Y_pred)  # deriv wrt c\n",
    "\n",
    "        # update the values\n",
    "        m = m - (learn_rate * D_m)  # updating m\n",
    "        c = c - (learn_rate * D_c)  # updating c\n",
    "        \n",
    "        iteration_count += 1\n",
    "\n",
    "    # print(f'Slope is: {m} and Intercept is: {c}')\n",
    "\n",
    "    # predictions based on the regression line calculated\n",
    "    Y_pred = (m * X) + c\n",
    "\n",
    "    # plot the information of the data points and the regression line\n",
    "    # plt.scatter(X, Y) \n",
    "    # plt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='red')  # regression line\n",
    "    # plt.show()\n",
    "    \n",
    "    return m, c, iteration_count, true_slope, true_int\n",
    "\n",
    "# randomized gradient descent\n",
    "def grad_reg_rand_threshold(X, Y, learning_rate, threshold):\n",
    "    \n",
    "    # get the \"true\" values first for comparison\n",
    "    true_slope, true_int = grad_reg_sk(X, Y)\n",
    "    \n",
    "    # initial slope and intercept guesses\n",
    "    m = 0\n",
    "    c = 0\n",
    "\n",
    "    learn_rate = learning_rate  # this is used as a learning rate base to get distribution numbers around\n",
    "    iteration_count = 0\n",
    "\n",
    "    n = int(len(X))  # for use in the derivatives later on\n",
    "\n",
    "    # gradient descent iterations until within threshold specified\n",
    "    while abs(true_slope - m) > threshold and abs(true_int - c) > threshold:\n",
    "\n",
    "        # current predicted value of Y\n",
    "        Y_pred = (m * X) + c\n",
    "\n",
    "        # derivatives\n",
    "        D_m = (-2/n) * sum(X * (Y - Y_pred))  # deriv wrt m\n",
    "        D_c = (-2/n) * sum(Y - Y_pred)  # deriv wrt c\n",
    "        \n",
    "        # get the random learning rate for this iteration\n",
    "        # this is where the randomness is being included - make sure that we get a positive number here so clip negative values\n",
    "        rnd_learn_rate = np.random.normal(learn_rate, learn_rate/2)\n",
    "        while rnd_learn_rate <= 0:\n",
    "            rnd_learn_rate = np.random.normal(learn_rate, learn_rate/2)\n",
    "\n",
    "        # update the values\n",
    "        m = m - (rnd_learn_rate * D_m)  # updating m\n",
    "        c = c - (rnd_learn_rate * D_c)  # updating c\n",
    "        \n",
    "        iteration_count += 1\n",
    "\n",
    "    # print(f'Slope is: {m} and Intercept is: {c}')\n",
    "\n",
    "    # predictions based on the regression line calculated\n",
    "    Y_pred = (m * X) + c\n",
    "\n",
    "    # plot the information of the data points and the regression line\n",
    "    # plt.scatter(X, Y) \n",
    "    # plt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='red')  # regression line\n",
    "    # plt.show()\n",
    "    \n",
    "    return m, c, iteration_count, true_slope, true_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842c751-376b-4ee2-bf78-424cfecf20d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run the comparisons\n",
    "reg_count = np.array([])\n",
    "rand_count = np.array([])\n",
    "\n",
    "threshold = 0.01\n",
    "\n",
    "# low rand and high rand are used when generating random X and Y data points\n",
    "low_rand_thresh = 0.8\n",
    "high_rand_thresh = 1.2\n",
    "\n",
    "# create an array of learning rates to go through\n",
    "learning_rate_arr_thresh = np.random.uniform(low=0.005, high=0.01, size=50)\n",
    "\n",
    "# run our gradient descent under each learning rate\n",
    "for learning_rate in learning_rate_arr_thresh:\n",
    "    \n",
    "    # generate random X and Y values to use in our gradient descent\n",
    "    X_pts, Y_pts = rand_points(low_rand_thresh, high_rand_thresh)\n",
    "    \n",
    "    # run the regular and random gradient descents to compare iterations\n",
    "    reg_threshold = grad_reg_threshold(X_pts, Y_pts, learning_rate, threshold)\n",
    "    rand_threshold = grad_reg_rand_threshold(X_pts, Y_pts, learning_rate, threshold)\n",
    "\n",
    "    # append the values to our arrays\n",
    "    reg_count = np.append(reg_count, reg_threshold[2])\n",
    "    rand_count = np.append(rand_count, rand_threshold[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e66a0-1d6a-4c18-9bac-a43d682b3b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the information for the errors of the slopes calculations\n",
    "# data of constant learning rate\n",
    "x5 = learning_rate_arr_thresh\n",
    "y5 = reg_count\n",
    "\n",
    "# data of random learning rate\n",
    "x6 = learning_rate_arr_thresh\n",
    "y6 = rand_count\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create scatter plots\n",
    "ax.scatter(x5, y5, color='blue', label='Constant Rate')\n",
    "ax.scatter(x6, y6, color='red', label='Random Rate')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Learning Rates')\n",
    "ax.set_ylabel(f'Iterations')\n",
    "ax.set_title('Scatter Plot on Iterations vs. Learning Rates')\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ef6f3-7a69-4e08-9305-e5b078c97b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# metrics for our comparison\n",
    "# checking number of times one beat the other\n",
    "iteration_compare = y6 - y5\n",
    "iteration_success_rate = np.count_nonzero(iteration_compare < 0) / len(iteration_compare)  # how many times the random algorithm won\n",
    "print(f'Random won {round(iteration_success_rate * 100, 2)}% of the time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64809afa-c8b6-46ef-83c9-73dd9074a059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
